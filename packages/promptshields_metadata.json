{
  "name": "promptshields",
  "version": "2.1.0",
  "summary": "Production-Grade LLM Security Framework - Protect against prompt injection, jailbreaks, and data leakage",
  "author": "Neuralchemy",
  "license": "MIT",
  "home_page": "https://github.com/Neural-alchemy/promptshield",
  "download_filename": "promptshields-2.1.0.tar.gz",
  "download_time": "2026-01-30T17:59:03.230054",
  "package_url": "https://pypi.org/project/promptshields/"
}