{
  "name": "promptshields",
  "version": "2.0.5",
  "summary": "Production-Grade LLM Security Framework - Protect against prompt injection, jailbreaks, and data leakage",
  "author": "Neuralchemy",
  "license": "MIT",
  "home_page": "https://github.com/Neural-alchemy/promptshield",
  "download_filename": "promptshields-2.0.5.tar.gz",
  "download_time": "2026-01-30T12:42:22.395553",
  "package_url": "https://pypi.org/project/promptshields/"
}